{
    "content": "\nIn a previous post, I announced plans to release an _alpha_ version of a complete system. I have a couple goals\nfor this release:\n\n1. It's been almost exactly three months since the last time I released an exercise. Each release forces me\n   to test deploying the system as a whole. I don't usually do this when I'm developing--instead I slowly\n   add things on to my already-deployed infrastructure. This keeps the work moving forward, but it comes with\n   a risk: that some of my iterative changes won't be deployable \"from scratch\" or that I've cut corners that\n   will require rework. Doing a release is an opportunity to identify and correct any issues like that.\n\n2. Once the code for the release comes together, I'd like to try to get five people besides me to deploy this\n   system _and attempt to use it_. I'll re-evaluate this goal once the release code is complete; I'll only solicit\n   volunteers if I really think the system can provide some usefulness to them as well as to me. But this goal\n   remains important whether or not I'm able to achieve it in this version; if I can't demonstrate usefulness-to-others,\n   at some point I'll need to move on to a different project.\n\nAlong with those goals, I have some _leniencies_--specific elements where I know more work is needed but release anyway.\nThis post is going to be a list of those places that I know about so far. In each case, I'm going to describe what is\nleft to do, the effect it will have on the system as a whole, and what the path to completing it might be.\n\n1. **Lambda@Edge Log Groups**: Tha auth system creates some log groups dynamically. A log group is a collection\n   of logs that are managed together in Cloudwatch, AWS's built-in log service. Because these log groups are created\n   dynamically, they are not given an expiration date. That means that over time they'll build up in the account.\n   The solution to this will be to write a lambda function that automatically sets expiration times on any autogenerated\n   log groups. The auth system doesn't log much; over the past three months I've accumulated around 3MB of these logs--the size\n   of a single uncompressed image from a nice phone camera.\n\n2. **Image Management and Deletion**: As a part of my goal to enable blog posts, I've included code for uploading images,\n   including them in posts, and publishing them along with the posts. However, I have not written a system for managing\n   the images themselves. I'm also not sure if I'm going to fully implement publlished-image deletion before the alpha release;\n   it may be the case that once published, images will be accessible via their direct URLs even if the relevant post is deleted.\n   This will also mean that we spend more storage space on images than necessary. I don't think this inefficiency is enough\n   to outweigh the value of the proof-of-concept. I also have some uncertainty about what \"image management\" should entail--for\n   instance, neither Medium nor Instagram lets you manage pictures independently of posts. Not being a photo-album guy myself,\n   I'm not sure how far to go in that direction, or if I could accurately anticipate how such a system should work. Open to\n   suggestions. A lot of the invisible complexity of having a photo management system _and_ a post-management system is\n   keeping track of interactions between the two--things like \"if you want to delete an image through the image management system,\n   but that image is being used in a blog post, how should that be communicated and what should happen?\" These types of\n   interactions can add a lot of unpleasant complexity if we're not careful.\n\n   __Update a couple of days later__: After thinking about it a little, I've decided that in this version, images will be tied\n   to the specific post where they are used. This means that when a post is published, its images are published with it, and\n   when it is _unpublished_ (when it is made nonpublic) its images are unpublished as well. This means that images are never \"shared\"\n   between posts--like on a service like instagram, if you want to put the same image in multiple posts you need to upload it multiple times.\n   This greatly helps to reduce the circumstances under which an image could remain published after its post was unpublished.\n   It's still a little inefficient.\n\n3. **Theming System**: The theming system for the blog is underdeveloped. A \"theming system\" on a blog refers to the way that you customize the\n   blog's appearance. I feel ok calling what I have a \"theming system,\" because each post is generated from a template. But\n   at this point the templates are deployed by terraform and not easily configurable. So for the alpha release, all the blogs\n   will look like this blog.\n\n4. **Trail link bugs and blog inefficiencies**: I've noticed some bugs in the way the trail links at the bottom of each post are generated. I haven't\n   made it a priority to fix them yet because I'm not sure how valuable those links are--in the logs, I don't see a lot\n   of evidence that people are using them. But maybe people aren't using them because they're broken. In any case, that's\n   more of a superficial thing, and I'm still focusing on foundations.\n\n   There's also still an efficiency issue where the blog stores the full text of entries in a dynamo database. I did this\n   because I wanted to enable RSS and Atom feeds quickly, but it really feels wrong to duplicate all that content in such a\n   user-opaque way--ideally, I want a non-practitioner to be able to use just the S3 UI to know everything they need to know\n   about the system, and letting dynamo have such a significant role interferes with that goal. I think the solution to this\n   will be to reduce the amount of content shown in the RSS and Atom feeds so that full text isn't needed in the database. I\n   may even need some convincing on the long-term usefulness of Atom and RSS under the assumtion that operator-controlled systems\n   ought to be the norm[^1].\n\n5. **Autosave, Archiving, and Plugin Boundaries**: One of the highest priorities of this system is to _preserve artifacts of human attention_.\n   That means that when the system owner does something that reflects a moment of their attention, such as starting to write a post\n   or uploading an image, the system should attempt to protect that thing from accidental loss in every likely scenario.\n\n   One of the _most_ likely scenarios, which I think we've all seen and been frustrated by in the past, is to be interacting\n   with a site when you suddenly get signed out because your session timed out. Session timeouts are a _defense in depth_ measure\n   to limit the damage that can be done by an adversary getting hold of a _session token_--the special password-like thing that\n   your browser uses to keep you signed in to a web app. I don't think that it would be safe to remove session timeouts, so our\n   data-preservation strategy needs to assume that they will happen, and the user _will_ sometimes be logged out while doing stuff.\n   \n   So the next question to ask is: how do we ensure that _even if_ the user is logged out when doing stuff, they don't lose more than\n   a few seconds of progress? One answer is _autosave_. This refers to the practice of saving the current state once every few seconds\n   while the user is editing it. That way, if the user gets logged out, they get a version from a few seconds before the logout happened,\n   not from the last time they saved manually. This was the first thing I tried.\n\n   Unfortunately, I ran into a couple of resource issues with that approach. The first was the interaction with the archive system. The\n   archive system is designed to back up _everything_ that lands in the system's storage. When combined with autosave, this means that each\n   version of a document that's being edited ends up being saved forever. When autosave runs every five seconds or so during editing, this\n   could mean saving hundreds of not-very-useful intermediate copies of a document. I think it's _likely_ that the archive system needs\n   to change somewhat, but I don't want to change it now[^2]. So I can either accept this issue or I can try to work around it within the blog system\n   itself. I decided on the latter.\n\n   It turns out that there _is_ a pretty straightforward solution to session timeouts happening on the browser. Browsers have a feature called [local storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage)\n   that lets website code save things between refreshes. That means that you can go to a website, do some stuff, refresh the page, and your data will still\n   be there. I experimented with this a bit and found a way to use it to preserve data even through session timeouts. This means that autosave could\n   mean \"save to local storage,\" and run every few seconds without misusing archive storage space. But then _another_ problem arises. My [security model for plugins](https://raphaelluckom.com/posts/isolation_proposal_001.html)\n   requires a boundary between the data from one plugin and the data from another. Local storage doesn't have a built-in way to enforce that kind of\n   boundary between different pages on a single website[^3].\n\n   So again, we can accept the problem or we can try to solve it. This time, I'm going to accept the problem for now. In the future, I propose\n   that each plugin will be able to create a lambda function for encryption and decryption. This lambda function can be the sole keeper of a \n   plugin-specific [symmetric key](https://en.wikipedia.org/wiki/Symmetric-key_algorithm) for encrypting and decrypting data. When a plugin wants to store\n   data in local storage, it can call its encryption endpoint with the plaintext, get the ciphertext, and store the ciphertext in local storage.\n   When it needs to decrypt local storage data, it can call the decrypt endpoint with the ciphertext to get the plaintext. The encryption / decryption\n   endpoints are restricted to a single plugin using the plugin isolation strategy described in the previous post.\n\n   Another item that I'm not going to resolve in time for this first release is standardizing plugin data-retention policies. A data-retention policy\n   specifies how long data will be saved. All data that gets uploaded gets stored in the archive system, presumably forever. But once data has been processed \n   by a plugin and saved to the archive, how long should we keep the original copy? The reason this question matters is that the archive is like a \n   storage unit--it's not convenient for things that will be used often. So the retention policy for the originals should be as long as we're likely to want to\n   actively modify the data. Does that mean a week? a month? a year? Different for different types of data? I'm not sure.\n\nI started writing this post on May 5 or May 6; I left it in progress for a few days so that I'd have time to capture different issues as I\nnotice them. I think it now captures most of the big todos, but others will certainly appear.\n\n[^1]: RSS and Atom are _feed_ formats--they're techniques for publishing a list of posts, articles, etc. that a website has made available. Their design\n      includes the assumption that a _reader_ program wants to get the full list of items, and that the publisher has a plan for managing the list of items\n      so that it doesn't get too long. That seems like _underspecification_ to me--there are enough unanswered questions about how things should work\n      that different programs that follow all the rules might be making different and contradictory assumptions about the questions that the rules\n      don't answer. For now I'm reserving judgment on whether these technologies are useful in this personal-social-media context or if different\n      solutions might be more appropriate.\n\n[^2]: The archive system is _global_--it's intended to support everything that this system does. The autosave feature I'm discussing is _local_--it's\n      a feature of a single plugin (the blog) to the overall system. When I'm considering changes to a global system, I want to see a global\n      reason for the changes--I want to see multiple plugins all \"agree\" on what the change should be. Here the situation is that _one_ plugin\n      is interacting badly with the archive system. If I change the archive system now, I run the risk of specializing it in a way that makes sense\n      for the blog plugin, but causes problems for other plugins I want to implement in the future. The work that I'm doing on the blog system\n      _does_ suggest that there are changes needed in the archive system, but I'd like to see those suggestions corroborated by my experience\n      with another plugin or two before I commit.\n\n[^3]: That is, one [origin](https://developer.mozilla.org/en-US/docs/Glossary/Origin) can't see the local storage from a different origin, but all the pages\n      from a given origin share a local storage namespace.\n\n",
    "frontMatter": {
        "author": "Raphael Luckom",
        "date": "2021-05-08T11:40:00.000Z",
        "createDate": "2021-05-08T11:40:00.000Z",
        "draft": false,
        "meta": {
            "trails": [
                "check-in",
                "alpha release"
            ]
        },
        "title": "Early May Check-in: Alpha release thoughts"
    },
    "id": "alpha_todos",
    "raw": "---\ntitle: \"Early May Check-in: Alpha release thoughts\"\nauthor: \"Raphael Luckom\"\ndate: 2021-05-08T11:40:00\ncreateDate: 2021-05-08T11:40:00\ndraft: false\nmeta:\n  trail:\n    - check-in\n    - alpha release\n---\n\nIn a previous post, I announced plans to release an _alpha_ version of a complete system. I have a couple goals\nfor this release:\n\n1. It's been almost exactly three months since the last time I released an exercise. Each release forces me\n   to test deploying the system as a whole. I don't usually do this when I'm developing--instead I slowly\n   add things on to my already-deployed infrastructure. This keeps the work moving forward, but it comes with\n   a risk: that some of my iterative changes won't be deployable \"from scratch\" or that I've cut corners that\n   will require rework. Doing a release is an opportunity to identify and correct any issues like that.\n\n2. Once the code for the release comes together, I'd like to try to get five people besides me to deploy this\n   system _and attempt to use it_. I'll re-evaluate this goal once the release code is complete; I'll only solicit\n   volunteers if I really think the system can provide some usefulness to them as well as to me. But this goal\n   remains important whether or not I'm able to achieve it in this version; if I can't demonstrate usefulness-to-others,\n   at some point I'll need to move on to a different project.\n\nAlong with those goals, I have some _leniencies_--specific elements where I know more work is needed but release anyway.\nThis post is going to be a list of those places that I know about so far. In each case, I'm going to describe what is\nleft to do, the effect it will have on the system as a whole, and what the path to completing it might be.\n\n1. **Lambda@Edge Log Groups**: Tha auth system creates some log groups dynamically. A log group is a collection\n   of logs that are managed together in Cloudwatch, AWS's built-in log service. Because these log groups are created\n   dynamically, they are not given an expiration date. That means that over time they'll build up in the account.\n   The solution to this will be to write a lambda function that automatically sets expiration times on any autogenerated\n   log groups. The auth system doesn't log much; over the past three months I've accumulated around 3MB of these logs--the size\n   of a single uncompressed image from a nice phone camera.\n\n2. **Image Management and Deletion**: As a part of my goal to enable blog posts, I've included code for uploading images,\n   including them in posts, and publishing them along with the posts. However, I have not written a system for managing\n   the images themselves. I'm also not sure if I'm going to fully implement publlished-image deletion before the alpha release;\n   it may be the case that once published, images will be accessible via their direct URLs even if the relevant post is deleted.\n   This will also mean that we spend more storage space on images than necessary. I don't think this inefficiency is enough\n   to outweigh the value of the proof-of-concept. I also have some uncertainty about what \"image management\" should entail--for\n   instance, neither Medium nor Instagram lets you manage pictures independently of posts. Not being a photo-album guy myself,\n   I'm not sure how far to go in that direction, or if I could accurately anticipate how such a system should work. Open to\n   suggestions. A lot of the invisible complexity of having a photo management system _and_ a post-management system is\n   keeping track of interactions between the two--things like \"if you want to delete an image through the image management system,\n   but that image is being used in a blog post, how should that be communicated and what should happen?\" These types of\n   interactions can add a lot of unpleasant complexity if we're not careful.\n\n   __Update a couple of days later__: After thinking about it a little, I've decided that in this version, images will be tied\n   to the specific post where they are used. This means that when a post is published, its images are published with it, and\n   when it is _unpublished_ (when it is made nonpublic) its images are unpublished as well. This means that images are never \"shared\"\n   between posts--like on a service like instagram, if you want to put the same image in multiple posts you need to upload it multiple times.\n   This greatly helps to reduce the circumstances under which an image could remain published after its post was unpublished.\n   It's still a little inefficient.\n\n3. **Theming System**: The theming system for the blog is underdeveloped. A \"theming system\" on a blog refers to the way that you customize the\n   blog's appearance. I feel ok calling what I have a \"theming system,\" because each post is generated from a template. But\n   at this point the templates are deployed by terraform and not easily configurable. So for the alpha release, all the blogs\n   will look like this blog.\n\n4. **Trail link bugs and blog inefficiencies**: I've noticed some bugs in the way the trail links at the bottom of each post are generated. I haven't\n   made it a priority to fix them yet because I'm not sure how valuable those links are--in the logs, I don't see a lot\n   of evidence that people are using them. But maybe people aren't using them because they're broken. In any case, that's\n   more of a superficial thing, and I'm still focusing on foundations.\n\n   There's also still an efficiency issue where the blog stores the full text of entries in a dynamo database. I did this\n   because I wanted to enable RSS and Atom feeds quickly, but it really feels wrong to duplicate all that content in such a\n   user-opaque way--ideally, I want a non-practitioner to be able to use just the S3 UI to know everything they need to know\n   about the system, and letting dynamo have such a significant role interferes with that goal. I think the solution to this\n   will be to reduce the amount of content shown in the RSS and Atom feeds so that full text isn't needed in the database. I\n   may even need some convincing on the long-term usefulness of Atom and RSS under the assumtion that operator-controlled systems\n   ought to be the norm[^1].\n\n5. **Autosave, Archiving, and Plugin Boundaries**: One of the highest priorities of this system is to _preserve artifacts of human attention_.\n   That means that when the system owner does something that reflects a moment of their attention, such as starting to write a post\n   or uploading an image, the system should attempt to protect that thing from accidental loss in every likely scenario.\n\n   One of the _most_ likely scenarios, which I think we've all seen and been frustrated by in the past, is to be interacting\n   with a site when you suddenly get signed out because your session timed out. Session timeouts are a _defense in depth_ measure\n   to limit the damage that can be done by an adversary getting hold of a _session token_--the special password-like thing that\n   your browser uses to keep you signed in to a web app. I don't think that it would be safe to remove session timeouts, so our\n   data-preservation strategy needs to assume that they will happen, and the user _will_ sometimes be logged out while doing stuff.\n   \n   So the next question to ask is: how do we ensure that _even if_ the user is logged out when doing stuff, they don't lose more than\n   a few seconds of progress? One answer is _autosave_. This refers to the practice of saving the current state once every few seconds\n   while the user is editing it. That way, if the user gets logged out, they get a version from a few seconds before the logout happened,\n   not from the last time they saved manually. This was the first thing I tried.\n\n   Unfortunately, I ran into a couple of resource issues with that approach. The first was the interaction with the archive system. The\n   archive system is designed to back up _everything_ that lands in the system's storage. When combined with autosave, this means that each\n   version of a document that's being edited ends up being saved forever. When autosave runs every five seconds or so during editing, this\n   could mean saving hundreds of not-very-useful intermediate copies of a document. I think it's _likely_ that the archive system needs\n   to change somewhat, but I don't want to change it now[^2]. So I can either accept this issue or I can try to work around it within the blog system\n   itself. I decided on the latter.\n\n   It turns out that there _is_ a pretty straightforward solution to session timeouts happening on the browser. Browsers have a feature called [local storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage)\n   that lets website code save things between refreshes. That means that you can go to a website, do some stuff, refresh the page, and your data will still\n   be there. I experimented with this a bit and found a way to use it to preserve data even through session timeouts. This means that autosave could\n   mean \"save to local storage,\" and run every few seconds without misusing archive storage space. But then _another_ problem arises. My [security model for plugins](https://raphaelluckom.com/posts/isolation_proposal_001.html)\n   requires a boundary between the data from one plugin and the data from another. Local storage doesn't have a built-in way to enforce that kind of\n   boundary between different pages on a single website[^3].\n\n   So again, we can accept the problem or we can try to solve it. This time, I'm going to accept the problem for now. In the future, I propose\n   that each plugin will be able to create a lambda function for encryption and decryption. This lambda function can be the sole keeper of a \n   plugin-specific [symmetric key](https://en.wikipedia.org/wiki/Symmetric-key_algorithm) for encrypting and decrypting data. When a plugin wants to store\n   data in local storage, it can call its encryption endpoint with the plaintext, get the ciphertext, and store the ciphertext in local storage.\n   When it needs to decrypt local storage data, it can call the decrypt endpoint with the ciphertext to get the plaintext. The encryption / decryption\n   endpoints are restricted to a single plugin using the plugin isolation strategy described in the previous post.\n\n   Another item that I'm not going to resolve in time for this first release is standardizing plugin data-retention policies. A data-retention policy\n   specifies how long data will be saved. All data that gets uploaded gets stored in the archive system, presumably forever. But once data has been processed \n   by a plugin and saved to the archive, how long should we keep the original copy? The reason this question matters is that the archive is like a \n   storage unit--it's not convenient for things that will be used often. So the retention policy for the originals should be as long as we're likely to want to\n   actively modify the data. Does that mean a week? a month? a year? Different for different types of data? I'm not sure.\n\nI started writing this post on May 5 or May 6; I left it in progress for a few days so that I'd have time to capture different issues as I\nnotice them. I think it now captures most of the big todos, but others will certainly appear.\n\n[^1]: RSS and Atom are _feed_ formats--they're techniques for publishing a list of posts, articles, etc. that a website has made available. Their design\n      includes the assumption that a _reader_ program wants to get the full list of items, and that the publisher has a plan for managing the list of items\n      so that it doesn't get too long. That seems like _underspecification_ to me--there are enough unanswered questions about how things should work\n      that different programs that follow all the rules might be making different and contradictory assumptions about the questions that the rules\n      don't answer. For now I'm reserving judgment on whether these technologies are useful in this personal-social-media context or if different\n      solutions might be more appropriate.\n\n[^2]: The archive system is _global_--it's intended to support everything that this system does. The autosave feature I'm discussing is _local_--it's\n      a feature of a single plugin (the blog) to the overall system. When I'm considering changes to a global system, I want to see a global\n      reason for the changes--I want to see multiple plugins all \"agree\" on what the change should be. Here the situation is that _one_ plugin\n      is interacting badly with the archive system. If I change the archive system now, I run the risk of specializing it in a way that makes sense\n      for the blog plugin, but causes problems for other plugins I want to implement in the future. The work that I'm doing on the blog system\n      _does_ suggest that there are changes needed in the archive system, but I'd like to see those suggestions corroborated by my experience\n      with another plugin or two before I commit.\n\n[^3]: That is, one [origin](https://developer.mozilla.org/en-US/docs/Glossary/Origin) can't see the local storage from a different origin, but all the pages\n      from a given origin share a local storage namespace.\n",
    "synthetic": {
        "renderedContent": "<p>In a previous post, I announced plans to release an <em>alpha</em> version of a complete system. I have a couple goals\nfor this release:</p>\n<ol>\n<li>\n<p>It's been almost exactly three months since the last time I released an exercise. Each release forces me\nto test deploying the system as a whole. I don't usually do this when I'm developing--instead I slowly\nadd things on to my already-deployed infrastructure. This keeps the work moving forward, but it comes with\na risk: that some of my iterative changes won't be deployable &quot;from scratch&quot; or that I've cut corners that\nwill require rework. Doing a release is an opportunity to identify and correct any issues like that.</p>\n</li>\n<li>\n<p>Once the code for the release comes together, I'd like to try to get five people besides me to deploy this\nsystem <em>and attempt to use it</em>. I'll re-evaluate this goal once the release code is complete; I'll only solicit\nvolunteers if I really think the system can provide some usefulness to them as well as to me. But this goal\nremains important whether or not I'm able to achieve it in this version; if I can't demonstrate usefulness-to-others,\nat some point I'll need to move on to a different project.</p>\n</li>\n</ol>\n<p>Along with those goals, I have some <em>leniencies</em>--specific elements where I know more work is needed but release anyway.\nThis post is going to be a list of those places that I know about so far. In each case, I'm going to describe what is\nleft to do, the effect it will have on the system as a whole, and what the path to completing it might be.</p>\n<ol>\n<li>\n<p><strong>Lambda@Edge Log Groups</strong>: Tha auth system creates some log groups dynamically. A log group is a collection\nof logs that are managed together in Cloudwatch, AWS's built-in log service. Because these log groups are created\ndynamically, they are not given an expiration date. That means that over time they'll build up in the account.\nThe solution to this will be to write a lambda function that automatically sets expiration times on any autogenerated\nlog groups. The auth system doesn't log much; over the past three months I've accumulated around 3MB of these logs--the size\nof a single uncompressed image from a nice phone camera.</p>\n</li>\n<li>\n<p><strong>Image Management and Deletion</strong>: As a part of my goal to enable blog posts, I've included code for uploading images,\nincluding them in posts, and publishing them along with the posts. However, I have not written a system for managing\nthe images themselves. I'm also not sure if I'm going to fully implement publlished-image deletion before the alpha release;\nit may be the case that once published, images will be accessible via their direct URLs even if the relevant post is deleted.\nThis will also mean that we spend more storage space on images than necessary. I don't think this inefficiency is enough\nto outweigh the value of the proof-of-concept. I also have some uncertainty about what &quot;image management&quot; should entail--for\ninstance, neither Medium nor Instagram lets you manage pictures independently of posts. Not being a photo-album guy myself,\nI'm not sure how far to go in that direction, or if I could accurately anticipate how such a system should work. Open to\nsuggestions. A lot of the invisible complexity of having a photo management system <em>and</em> a post-management system is\nkeeping track of interactions between the two--things like &quot;if you want to delete an image through the image management system,\nbut that image is being used in a blog post, how should that be communicated and what should happen?&quot; These types of\ninteractions can add a lot of unpleasant complexity if we're not careful.</p>\n<p><strong>Update a couple of days later</strong>: After thinking about it a little, I've decided that in this version, images will be tied\nto the specific post where they are used. This means that when a post is published, its images are published with it, and\nwhen it is <em>unpublished</em> (when it is made nonpublic) its images are unpublished as well. This means that images are never &quot;shared&quot;\nbetween posts--like on a service like instagram, if you want to put the same image in multiple posts you need to upload it multiple times.\nThis greatly helps to reduce the circumstances under which an image could remain published after its post was unpublished.\nIt's still a little inefficient.</p>\n</li>\n<li>\n<p><strong>Theming System</strong>: The theming system for the blog is underdeveloped. A &quot;theming system&quot; on a blog refers to the way that you customize the\nblog's appearance. I feel ok calling what I have a &quot;theming system,&quot; because each post is generated from a template. But\nat this point the templates are deployed by terraform and not easily configurable. So for the alpha release, all the blogs\nwill look like this blog.</p>\n</li>\n<li>\n<p><strong>Trail link bugs and blog inefficiencies</strong>: I've noticed some bugs in the way the trail links at the bottom of each post are generated. I haven't\nmade it a priority to fix them yet because I'm not sure how valuable those links are--in the logs, I don't see a lot\nof evidence that people are using them. But maybe people aren't using them because they're broken. In any case, that's\nmore of a superficial thing, and I'm still focusing on foundations.</p>\n<p>There's also still an efficiency issue where the blog stores the full text of entries in a dynamo database. I did this\nbecause I wanted to enable RSS and Atom feeds quickly, but it really feels wrong to duplicate all that content in such a\nuser-opaque way--ideally, I want a non-practitioner to be able to use just the S3 UI to know everything they need to know\nabout the system, and letting dynamo have such a significant role interferes with that goal. I think the solution to this\nwill be to reduce the amount of content shown in the RSS and Atom feeds so that full text isn't needed in the database. I\nmay even need some convincing on the long-term usefulness of Atom and RSS under the assumtion that operator-controlled systems\nought to be the norm<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup>.</p>\n</li>\n<li>\n<p><strong>Autosave, Archiving, and Plugin Boundaries</strong>: One of the highest priorities of this system is to <em>preserve artifacts of human attention</em>.\nThat means that when the system owner does something that reflects a moment of their attention, such as starting to write a post\nor uploading an image, the system should attempt to protect that thing from accidental loss in every likely scenario.</p>\n<p>One of the <em>most</em> likely scenarios, which I think we've all seen and been frustrated by in the past, is to be interacting\nwith a site when you suddenly get signed out because your session timed out. Session timeouts are a <em>defense in depth</em> measure\nto limit the damage that can be done by an adversary getting hold of a <em>session token</em>--the special password-like thing that\nyour browser uses to keep you signed in to a web app. I don't think that it would be safe to remove session timeouts, so our\ndata-preservation strategy needs to assume that they will happen, and the user <em>will</em> sometimes be logged out while doing stuff.</p>\n<p>So the next question to ask is: how do we ensure that <em>even if</em> the user is logged out when doing stuff, they don't lose more than\na few seconds of progress? One answer is <em>autosave</em>. This refers to the practice of saving the current state once every few seconds\nwhile the user is editing it. That way, if the user gets logged out, they get a version from a few seconds before the logout happened,\nnot from the last time they saved manually. This was the first thing I tried.</p>\n<p>Unfortunately, I ran into a couple of resource issues with that approach. The first was the interaction with the archive system. The\narchive system is designed to back up <em>everything</em> that lands in the system's storage. When combined with autosave, this means that each\nversion of a document that's being edited ends up being saved forever. When autosave runs every five seconds or so during editing, this\ncould mean saving hundreds of not-very-useful intermediate copies of a document. I think it's <em>likely</em> that the archive system needs\nto change somewhat, but I don't want to change it now<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">[2]</a></sup>. So I can either accept this issue or I can try to work around it within the blog system\nitself. I decided on the latter.</p>\n<p>It turns out that there <em>is</em> a pretty straightforward solution to session timeouts happening on the browser. Browsers have a feature called <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage\" target=\"_blank\">local storage</a>\nthat lets website code save things between refreshes. That means that you can go to a website, do some stuff, refresh the page, and your data will still\nbe there. I experimented with this a bit and found a way to use it to preserve data even through session timeouts. This means that autosave could\nmean &quot;save to local storage,&quot; and run every few seconds without misusing archive storage space. But then <em>another</em> problem arises. My <a href=\"https://raphaelluckom.com/posts/isolation_proposal_001.html\" target=\"_blank\">security model for plugins</a>\nrequires a boundary between the data from one plugin and the data from another. Local storage doesn't have a built-in way to enforce that kind of\nboundary between different pages on a single website<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">[3]</a></sup>.</p>\n<p>So again, we can accept the problem or we can try to solve it. This time, I'm going to accept the problem for now. In the future, I propose\nthat each plugin will be able to create a lambda function for encryption and decryption. This lambda function can be the sole keeper of a\nplugin-specific <a href=\"https://en.wikipedia.org/wiki/Symmetric-key_algorithm\" target=\"_blank\">symmetric key</a> for encrypting and decrypting data. When a plugin wants to store\ndata in local storage, it can call its encryption endpoint with the plaintext, get the ciphertext, and store the ciphertext in local storage.\nWhen it needs to decrypt local storage data, it can call the decrypt endpoint with the ciphertext to get the plaintext. The encryption / decryption\nendpoints are restricted to a single plugin using the plugin isolation strategy described in the previous post.</p>\n<p>Another item that I'm not going to resolve in time for this first release is standardizing plugin data-retention policies. A data-retention policy\nspecifies how long data will be saved. All data that gets uploaded gets stored in the archive system, presumably forever. But once data has been processed\nby a plugin and saved to the archive, how long should we keep the original copy? The reason this question matters is that the archive is like a\nstorage unit--it's not convenient for things that will be used often. So the retention policy for the originals should be as long as we're likely to want to\nactively modify the data. Does that mean a week? a month? a year? Different for different types of data? I'm not sure.</p>\n</li>\n</ol>\n<p>I started writing this post on May 5 or May 6; I left it in progress for a few days so that I'd have time to capture different issues as I\nnotice them. I think it now captures most of the big todos, but others will certainly appear.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>RSS and Atom are <em>feed</em> formats--they're techniques for publishing a list of posts, articles, etc. that a website has made available. Their design\nincludes the assumption that a <em>reader</em> program wants to get the full list of items, and that the publisher has a plan for managing the list of items\nso that it doesn't get too long. That seems like <em>underspecification</em> to me--there are enough unanswered questions about how things should work\nthat different programs that follow all the rules might be making different and contradictory assumptions about the questions that the rules\ndon't answer. For now I'm reserving judgment on whether these technologies are useful in this personal-social-media context or if different\nsolutions might be more appropriate. <a href=\"#fnref1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>The archive system is <em>global</em>--it's intended to support everything that this system does. The autosave feature I'm discussing is <em>local</em>--it's\na feature of a single plugin (the blog) to the overall system. When I'm considering changes to a global system, I want to see a global\nreason for the changes--I want to see multiple plugins all &quot;agree&quot; on what the change should be. Here the situation is that <em>one</em> plugin\nis interacting badly with the archive system. If I change the archive system now, I run the risk of specializing it in a way that makes sense\nfor the blog plugin, but causes problems for other plugins I want to implement in the future. The work that I'm doing on the blog system\n<em>does</em> suggest that there are changes needed in the archive system, but I'd like to see those suggestions corroborated by my experience\nwith another plugin or two before I commit. <a href=\"#fnref2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>That is, one <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/Origin\" target=\"_blank\">origin</a> can't see the local storage from a different origin, but all the pages\nfrom a given origin share a local storage namespace. <a href=\"#fnref3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n",
        "trails": [
            "https://%24%7Bdomain_name%7D/trails/check-in.html",
            "https://%24%7Bdomain_name%7D/trails/alpha%20release.html"
        ],
        "url": "https://%24%7Bdomain_name%7D/posts/alpha_todos.html"
    }
}
